{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Entropy\n",
    "\n",
    "Information entropy is the average rate at which information is produced by a stochastic source of data. The measure of information entropy associated with each possible data value\n",
    "\n",
    "![](../resources/images/tex_entropy.png)\n",
    "\n",
    " \n",
    "# Cross Entropy\n",
    "\n",
    "In information theory, the cross entropy between two probability distributions *p* and *q* over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an \"artificial\" probability distribution *q*, rather than the \"true\" distribution *p*.\n",
    "\n",
    "![](../resources/images/venn_cross_entropy.png)\n",
    "\n",
    "![](../resources/images/tex_cross_entropy.png)\n",
    "\n",
    "# Relative Entropy \n",
    "### ***(Kullback-Liebler Divergence)***\n",
    "\n",
    "\n",
    "In mathematical statistics, the Kullbackâ€“Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.\n",
    "\n",
    "![](../resources/images/venn_relative_entropy.png)\n",
    "\n",
    "![](../resources/images/tex_relative_entropy.png)\n",
    "\n",
    "# Join Entropy\n",
    "\n",
    "In information theory, joint entropy is a measure of the uncertainty associated with a set of variables.\n",
    "\n",
    "![](../resources/images/venn_joint_entropy.png)\n",
    "\n",
    "![](../resources/images/tex_joint_entropy.png)\n",
    "\n",
    "\n",
    "# Conditional Entropy\n",
    "\n",
    "n information theory, the conditional entropy (or equivocation) quantifies the amount of information needed to describe the outcome of a random variable *y* given that the value of another random variable *x* is known.\n",
    "\n",
    "![](../resources/images/venn_conditional_entropy.png)\n",
    "\n",
    "![](../resources/images/tex_conditional_entropy.png)\n",
    "\n",
    "# Mutual Information\n",
    "\n",
    "In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables.\n",
    "\n",
    "![](../resources/images/venn_mutual_information.png)\n",
    "\n",
    "![](../resources/images/tex_mutual_information.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
