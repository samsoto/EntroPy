# Entropy

Information entropy is the average rate at which information is produced by a stochastic source of data. The measure of information entropy associated with each possible data value

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/tex_entropy.png)

 
# Cross Entropy

In information theory, the cross entropy between two probability distributions *p* and *q* over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "artificial" probability distribution *q*, rather than the "true" distribution *p*.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-cross-entropy.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/tex_cross_entropy.png)

# Kullback-Liebler Divergence

In mathematical statistics, the Kullbackâ€“Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-relative-entropy.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/tex_relative_entropy.png)

# Join Entropy

In information theory, joint entropy is a measure of the uncertainty associated with a set of variables.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-mutual-information.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/tex-mutual-information.png)


# Conditional Entropy

n information theory, the conditional entropy (or equivocation) quantifies the amount of information needed to describe the outcome of a random variable *y* given that the value of another random variable *x* is known.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn_conditional_entropy.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/tex_conditional_entropy.png)

# Mutual Information

In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-mutual-information.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/tex-mutual-information.png)

