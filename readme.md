# Entropy Wiki

    http://www.sciweavers.org/free-online-latex-equation-editor
    \bg_black \large H(X)=\sum_{i}p(x_{i})\log_{2}\frac{1}{p(x_{i})}

![entropy-venn-diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/VennInfo3Var.svg/256px-VennInfo3Var.svg.png)


# Entropy

Information entropy is the average rate at which information is produced by a stochastic source of data. The measure of information entropy associated with each possible data value

<div style="background-color:rgba(255, 255, 255, 0.8); text-align:center; vertical-align: middle; padding:10px 0;">
<img src="http://www.sciweavers.org/upload/Tex2Img_1544155454/eqn.png" />
 </div>

 
# Cross Entropy

In information theory, the cross entropy between two probability distributions *p* and *q* over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "artificial" probability distribution *q*, rather than the "true" distribution *p*.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/cross_ven.png)

<div style="background-color:rgba(255, 255, 255, 0.8); text-align:center; vertical-align: middle; padding:10px 0;">
<img src="http://www.sciweavers.org/upload/Tex2Img_1544155803/eqn.png" />
</div>

# Kullback-Liebler Divergence

In mathematical statistics, the Kullbackâ€“Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.

<div style="background-color:rgba(255, 255, 255, 0.8); text-align:center; vertical-align: middle; padding:10px 0;">
<img src="http://www.sciweavers.org/upload/Tex2Img_1544155866/eqn.png" />
 </div>

# Conditional Entropy

n information theory, the conditional entropy (or equivocation) quantifies the amount of information needed to describe the outcome of a random variable *y* given that the value of another random variable *x* is known.

<div style="background-color:rgba(255, 255, 255, 0.8); text-align:center; vertical-align: middle; padding:10px 0;">
<img src="http://www.sciweavers.org/upload/Tex2Img_1544155968/eqn.png" />
 </div>

# Mutual Information

In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables.

<div style="background-color:rgba(255, 255, 255, 0.8); text-align:center; vertical-align: middle; padding:10px 0;">
<img src="http://www.sciweavers.org/upload/Tex2Img_1544156025/eqn.png" />
 </div>

