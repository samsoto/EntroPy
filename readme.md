# Entropy Wiki

    http://www.sciweavers.org/free-online-latex-equation-editor
    \bg_black \large H(X)=\sum_{i}p(x_{i})\log_{2}\frac{1}{p(x_{i})}

![entropy-venn-diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/VennInfo3Var.svg/256px-VennInfo3Var.svg.png)


# Entropy

Information entropy is the average rate at which information is produced by a stochastic source of data. The measure of information entropy associated with each possible data value

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/equ_entropy.png)

 
# Cross Entropy

In information theory, the cross entropy between two probability distributions *p* and *q* over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an "artificial" probability distribution *q*, rather than the "true" distribution *p*.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-cross_entropy.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/equ_cross.png)

# Kullback-Liebler Divergence

In mathematical statistics, the Kullbackâ€“Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-kl-divergence.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/equ_kl_divergence.png)

# Conditional Entropy

n information theory, the conditional entropy (or equivocation) quantifies the amount of information needed to describe the outcome of a random variable *y* given that the value of another random variable *x* is known.

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/venn-cond_entropy.png)

![](https://github.com/samsoto/EntroPy/blob/master/resources/images/equ_cond_entropy.png)

# Mutual Information

In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables.


![](https://github.com/samsoto/EntroPy/blob/master/resources/images/equ_mi.png)

