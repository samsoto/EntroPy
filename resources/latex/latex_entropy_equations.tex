Entropy - H(X)=\sum_{i}p(x_i)\log\frac{1}{p(x_i)}
Cross Entropy - H_{cross}(X,Y)=\sum_{i}p(x_i)\log\frac{1}{p(y_i)}
KL-Divergence - D_{KL}(X||Y)=\sum_{i}p(x_i)\log\frac{p(x_i)}{p(y_i)}
Joint Entropy - H(X,Y)=\sum_{i,j}p(x_i,y_j)\log\frac{1}{p(x_i,y_j)}
Conditional Entropy - H(X|Y)=\sum_{i,j}p(x_i,y_j)\log\frac{p(y_i)}{p(x_i,y_j)}
Mutual Information - I(X;Y)=\sum_{i,j}p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}
\mathbf{H(X)=\sum_{i}^{n}p(x_i)*(Number\:of\:Yes/No\:Questions)}
\mathbf{H(X)=\sum_{i}^{n}p(x_i)*log_2\frac{1}{p(x_i)}}



|                        **Example 1**                       	|       	|                       **Example 2**                      	|
|:----------------------------------------------------------:	|-------	|:--------------------------------------------------------:	|
| ![](./resources/images/intro/symbol_a_alice_bob.png)       	|       	| ![](./resources/images/intro/symbol_b_alice_bob.png)     	|
| ![](./resources/images/intro/symbol_a_questons.png)        	|       	| ![](./resources/images/intro/symbol_b_questons.png)      	|
| ![](./resources/images/intro/symbol_a_num_questions_2.png) 	|       	| ![](./resources/images/intro/symbol_b_num_questions.png) 	|
