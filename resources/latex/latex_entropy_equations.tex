Entropy - H(X)=\sum_{i}p(x_i)\log\frac{1}{p(x_i)}
Cross Entropy - H_{cross}(X,Y)=\sum_{i}p(x_i)\log\frac{1}{p(y_i)}
KL-Divergence - D_{KL}(X||Y)=\sum_{i}p(x_i)\log\frac{p(x_i)}{p(y_i)}
Joint Entropy - H(X,Y)=\sum_{i,j}p(x_i,y_j)\log\frac{1}{p(x_i,y_j)}
Conditional Entropy - H(X|Y)=\sum_{i,j}p(x_i,y_j)\log\frac{p(y_i)}{p(x_i,y_j)}
Mutual Information - I(X;Y)=\sum_{i,j}p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)p(y_j)}
\mathbf{H(X)=\sum_{i}^{n}p(x_i)*(Number\:of\:Yes/No\:Questions)}
\mathbf{H(X)=\sum_{i}^{n}p(x_i)*log_2\frac{1}{p(x_i)}}